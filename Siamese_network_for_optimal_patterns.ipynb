{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pRRDzsxJQW7J",
        "IVH_Mpu1fLFE",
        "h1zMuZpXW8g9",
        "t_Iu9PnXZYo_"
      ],
      "authorship_tag": "ABX9TyNW71Tt7Za17HyrRxRcrOaS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jialubio/SiameseNetwork_BrachingPatterns/blob/main/Siamese_network_for_optimal_patterns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRRDzsxJQW7J"
      },
      "source": [
        "#0. Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-OmTkx_QHSE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "f4ae8b61-9798-4add-845b-933fcd51349c"
      },
      "source": [
        "# Up load necessary files\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e530b46adfe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'imresize'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46un7FpwTtWM"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejyFABpDTwRt"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "!pip install git+https://github.com/raghakot/keras-vis.git -U\n",
        "!pip install -I scipy==1.2.*\n",
        "\n",
        "from vis.visualization import visualize_saliency,visualize_cam\n",
        "from vis.utils import utils\n",
        "\n",
        "import keras\n",
        "from keras import activations,models\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dense, Dropout, Flatten,Activation,Conv2D, MaxPooling2D\n",
        "import tensorflow.compat.v1 as tf\n",
        "from tensorflow.compat.v1.keras import backend\n",
        "print(tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import ImageOps,Image\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import seaborn as sn\n",
        "import os\n",
        "import math\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "from utils_saliency_map import special_csv_reader,label_str_to_dec, label_dec_to_binary,\\\n",
        "     label_binary_to_dec, class_counter,data_distribution,image_set,\\\n",
        "     train_valid_loss_plot,train_valid_acc_plot,pred_standard,save_saliency_map,\\\n",
        "     save_saliency_map,create_circular_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqtTQ472T6wE"
      },
      "source": [
        "# 1. Upload files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PktNwvaT7s4"
      },
      "source": [
        "# Unzip dataset\n",
        "!unzip -q '/content/gdrive/My Drive/' \\\n",
        "-d '/content/dataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7j97gGaUCu1"
      },
      "source": [
        "# Path\n",
        "path = \"/content/dataset/\"\n",
        "path_final = path + \"final/\"\n",
        "path_plaintext = path\n",
        "path_out = path + \"/output/\"\n",
        "!mkdir \"/content/dataset/output\"\n",
        "\n",
        "filename = path_plaintext + 'labels.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk5c7r07WH28"
      },
      "source": [
        "char_list = range(1,4)\n",
        "char_list = [str(i) for i in char_list]\n",
        "int2char  = dict(enumerate(char_list))\n",
        "char2int  = {char: ind for ind, char in int2char.items()}\n",
        "\n",
        "num_classes = len(char_list)\n",
        "num_replicate = 1000\n",
        "dataset_size = num_classes*num_replicate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW1BQI-tWyDt"
      },
      "source": [
        "# 2.Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvESM4o3WxG4"
      },
      "source": [
        "# Labels\n",
        "# Convert alphabetic labels into decimal and binary\n",
        "y = []\n",
        "with open(filename,\"r\") as csvFile:\n",
        "    for row in csvFile:\n",
        "        y.append(row[:-1])\n",
        "\n",
        "y = np.asarray(y)\n",
        "y_dec = label_str_to_dec(y[0:dataset_size],char2int).reshape(dataset_size,1)\n",
        "y_binary = label_dec_to_binary(y_dec[0:dataset_size],num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U3_3BilW138"
      },
      "source": [
        "# Load images and resize\n",
        "ResizedHeight = 80\n",
        "train_data = image_set(path_final,'FImg_ID_',\n",
        "                       dataset_size,\n",
        "                       resize=True,\n",
        "                       height=ResizedHeight,\n",
        "                       width=ResizedHeight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uRf4bzKe_TP"
      },
      "source": [
        "# 3. Create dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm6icUs8fOvz"
      },
      "source": [
        "## Construct image pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxBcCTwietxI"
      },
      "source": [
        "def get_random_image(img_groups, group_names, gid):\n",
        "    gname = group_names[gid]\n",
        "    photos = img_groups[gname]\n",
        "    pid = np.random.choice(np.arange(len(photos)), size=1)[0]\n",
        "    pname = photos[pid]\n",
        "    return gname + pname + \".jpg\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPH7Pszmeu8E"
      },
      "source": [
        "def create_triples(image_dir):\n",
        "    img_groups = {}\n",
        "    for img_file in os.listdir(image_dir):\n",
        "        prefix, suffix = img_file.split(\".\")\n",
        "        gid, pid = prefix[0:4], prefix[4:]\n",
        "        if img_groups.has_key(gid):\n",
        "            img_groups[gid].append(pid)\n",
        "        else:\n",
        "            img_groups[gid] = [pid]\n",
        "\n",
        "    pos_triples, neg_triples = [], []\n",
        "\n",
        "    # positive pairs are any combination of images in same group\n",
        "    for key in img_groups.keys():\n",
        "        triples = [(key + x[0] + \".jpg\", key + x[1] + \".jpg\", 1)\n",
        "                 for x in itertools.combinations(img_groups[key], 2)]\n",
        "        pos_triples.extend(triples)\n",
        "\n",
        "    # need equal number of negative examples\n",
        "    group_names = list(img_groups.keys())\n",
        "    for i in range(len(pos_triples)):\n",
        "        g1, g2 = np.random.choice(np.arange(len(group_names)), size=2, replace=False)\n",
        "        left = get_random_image(img_groups, group_names, g1)\n",
        "        right = get_random_image(img_groups, group_names, g2)\n",
        "        neg_triples.append((left, right, 0))\n",
        "\n",
        "    pos_triples.extend(neg_triples)\n",
        "    shuffle(pos_triples)\n",
        "\n",
        "    return pos_triples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmXb2NobngVN"
      },
      "source": [
        "def make_img_pairs(n,num_classes,y):\n",
        "  # output: [(img_idx_1, img_idx_2, label)]\n",
        "  # img_idx: index of images in the original image stack\n",
        "  # label: 1 - same class, 0 - different class\n",
        "\n",
        "  # get image indices of each class\n",
        "  img_classes = {}\n",
        "  for i in num_classes:\n",
        "    indices = np.where(y[:, i] == 1.)[i]\n",
        "    img_classes[i] = indices\n",
        "\n",
        "  sim_img_pairs,dis_img_pairs = [],[]\n",
        "\n",
        "  # make similar image pairs\n",
        "  for key in img_classes.keys():\n",
        "    new_pair = [(x[0], x[1], 1) \\\n",
        "               for x in itertools.combinations(img_classes[key], 2)]\n",
        "\n",
        "    sim_img_pairs.extend(new_pair)\n",
        "\n",
        "  # make dissimilar image pairs\n",
        "  class_names = list(img_classes.keys())\n",
        "    for i in range(len(pos_triples)):\n",
        "        class1, class2 = np.random.choice(np.arange(num_classes), size=2, replace=False) + 1\n",
        "        idx1 = np.random.choice(np.arange(len(img_classes[class1])))\n",
        "        idx2 = np.random.choice(np.arange(len(img_classes[class2])))\n",
        "        left = img_classes(class1)[idx1]\n",
        "        right = img_classes(class2)[idx2]\n",
        "        dis_img_pairs.append((left, right, 0))\n",
        "\n",
        "  # Combine and shuffle the image pairs\n",
        "  sim_img_pairs.extend(dis_img_pairs)\n",
        "  shuffle(sim_img_pairs)\n",
        "  return img_pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywm_aRXu5mSL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72993a9f-cbb2-443e-a518-2e1bc148a0fb"
      },
      "source": [
        "img_classes = {1:(1,2,3,6,7,9), 2:(1,2,3,6,7,9)}\n",
        "num_classes = 2\n",
        "class1, class2 = np.random.choice(np.arange(num_classes), size=2, replace=False) +1\n",
        "\n",
        "\n",
        "\n",
        "idx1 = np.random.choice(np.arange(l1))\n",
        "idx2 = np.random.choice(np.arange(l2))\n",
        "print(idx1,idx2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXSqpx_t7OQV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "d51c7fbb-c937-4d22-b433-b1b613dbb784"
      },
      "source": [
        "img_classes.get(class1)\n",
        "len(img_classes.get(class1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-80877a9d7eec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg27MqFge36E"
      },
      "source": [
        "triples_data = create_triples(IMAGE_DIR)\n",
        "\n",
        "print(\"# image triples:\", len(triples_data))\n",
        "[x for x in triples_data[0:5]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVH_Mpu1fLFE"
      },
      "source": [
        "## Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yru3yGhJW3cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "f4b521e7-c22c-438e-deb8-b496b526034b"
      },
      "source": [
        "# Split training, test and validation sets\n",
        "X_data, X_test, y_data, y_test = train_test_split(train_data.image_stack, y_binary,stratify=y_binary,test_size=0.1,shuffle=True, random_state=25)\n",
        "if split_idx == 0:\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_data,\n",
        "                                                      y_data,\n",
        "                                                      stratify=y_data,\n",
        "                                                      test_size=0.11111,\n",
        "                                                      shuffle=True,\n",
        "                                                      random_state=25)\n",
        "else:\n",
        "    X_data, X_val, y_data, y_val = train_test_split(X_data,\n",
        "                                                    y_data,\n",
        "                                                    stratify=y_data,\n",
        "                                                    test_size=0.11111,\n",
        "                                                    shuffle=True,\n",
        "                                                    random_state=25)\n",
        "    seg_ratio = [0.5,0.75,0.875,0.9375,0.96875,0.9875,0.9975]\n",
        "    X_train, X_left, y_train, y_left = train_test_split(X_data,\n",
        "                                                        y_data,\n",
        "                                                        stratify=y_data,\n",
        "                                                        test_size=seg_ratio[split_idx-1],\n",
        "                                                        shuffle=True,\n",
        "                                                        random_state=25)\n",
        "\n",
        "trainsize = X_train.shape[0]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_test_split' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3883df47421f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Split training, test and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_binary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_binary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msplit_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     X_train, X_val, y_train, y_val = train_test_split(X_data, \n\u001b[1;32m      5\u001b[0m                                                       \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-eLpAGrW5XZ"
      },
      "source": [
        "# Reshape dataset\n",
        "X_train = X_train.reshape((len(X_train),ResizedHeight,ResizedHeight,1))\n",
        "X_test = X_test.reshape((len(X_test),ResizedHeight,ResizedHeight,1))\n",
        "X_val = X_val.reshape((len(X_val),ResizedHeight,ResizedHeight,1))\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1zMuZpXW8g9"
      },
      "source": [
        "# 4. Siamese network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycM--bmpW-Gv"
      },
      "source": [
        "# Build model\n",
        "initializer=keras.initializers.glorot_normal(seed=27) # change the seed\n",
        "model = keras.Sequential()\n",
        "# model.add(keras.layers.Reshape((ResizedHeight,ResizedHeight,1), input_shape=(ResizedHeight,ResizedHeight,)))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=(5, 5),padding='same',kernel_initializer=initializer)) #3/5/7 ,activation='relu'\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(10, 10))) # 40,2\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=(5, 5),padding='same',kernel_initializer=initializer)) #3/5/7 ,activation='relu'\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(8, 8))) # 40,2\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(keras.layers.Reshape((128*1*1,), input_shape=(1,1,128)))\n",
        "model.add(Dense(50, activation='relu',kernel_initializer=initializer))\n",
        "model.add(Dense(num_classes, activation='softmax',kernel_initializer=initializer))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.001, nesterov=False),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMWPVc3zXFjV"
      },
      "source": [
        "earlystopping_loss = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                        min_delta=0.00001,\n",
        "                                                        patience=30,\n",
        "                                                        verbose=0,\n",
        "                                                        mode='auto',\n",
        "                                                        baseline=None,\n",
        "                                                        restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMe00uUbXIag"
      },
      "source": [
        "# Train with early stopping\n",
        "for i in range(0, 1):\n",
        "    history = model.fit(X_train,\n",
        "                      y_train,\n",
        "                      epochs = 50,\n",
        "                      batch_size=16,\n",
        "                      verbose=1,\n",
        "                      validation_data=(X_val, y_val),\n",
        "                      callbacks=[earlystopping_loss])\n",
        "\n",
        "    a = bool( earlystopping_loss.stopped_epoch >= 30 and earlystopping_loss.stopped_epoch < 100)\n",
        "    if i == 0:\n",
        "        stopped_epoch = [earlystopping_loss.stopped_epoch]\n",
        "    else:\n",
        "        next_epoch    = [earlystopping_loss.stopped_epoch]\n",
        "        stopped_epoch = np.concatenate((stopped_epoch,next_epoch), axis = 0)\n",
        "    if a:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_Iu9PnXZYo_"
      },
      "source": [
        "# 5. Predict similarity score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Lbt7Kw_ZaLg"
      },
      "source": [
        "# Predict\n",
        "pred_y_train = pred_standard(model2.predict(X_train, batch_size=None, verbose=0, steps=None))\n",
        "pred_y_test  = pred_standard(model2.predict(X_test,  batch_size=None, verbose=0, steps=None))\n",
        "pred_y_val   = pred_standard(model2.predict(X_val,   batch_size=None, verbose=0, steps=None))\n",
        "\n",
        "# Compute and save accuracy\n",
        "acc_train = accuracy_score(y_train, pred_y_train)\n",
        "acc_test  = accuracy_score(y_test, pred_y_test)\n",
        "acc_val   = accuracy_score(y_val, pred_y_val)\n",
        "accuracy_list = [acc_train,acc_val,acc_test]\n",
        "print(accuracy_list)\n",
        "\n",
        "#filename4 = path_out + Simulation_Parameters + \"class_\" + str(num_classes) + \"_trainsize_\" + str(trainsize) + \"_accuracy\" + \".txt\"\n",
        "#np.savetxt(filename4, accuracy_list, delimiter=',', fmt = '%f')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}